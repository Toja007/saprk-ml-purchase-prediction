# -*- coding: utf-8 -*-
"""spark-ml-purchase-prdiction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LLw21D9Wke36wpmwjH2xpaB0J0EjZJOP
"""

# Commented out IPython magic to ensure Python compatibility.
#importing libraries
from pyspark.sql import SparkSession
import pyspark.sql.types as tp
from pyspark.sql import functions as f
import matplotlib.pyplot as plt
# %matplotlib inline

#initializing spark
spark = SparkSession.builder.appName("purchase_predictor").getOrCreate()



# loading data
purchase_train = spark.read.csv("/content/train.csv", header =True, inferSchema = True)
purchase_test = spark.read.csv("/content/test.csv", header =True, inferSchema = True)

purchase_train.show()

purchase_test.show()

purchase_train.printSchema()

#Average purchase amount
average_purchase = purchase_train.groupBy("Product_ID").agg(f.avg("purchase").alias("Avarage_purchase"))
average_purchase.show()

average_purchase.orderBy("Avarage_purchase", ascending=False).show()

"""COUNTING AND REMOVING NULL VALUES"""

#count null values
purchase_train.select([f.count(f.when(f.isnull(c), c)).alias(c) for c in purchase_train.columns]).show()

purchase_test.select([f.count(f.when(f.isnull(c), c)).alias(c) for c in purchase_test.columns]).show()

# count of cat2 and cat2
purchase_train.groupBy("Product_Category_2").agg(f.count("Product_Category_2")).orderBy("count(Product_Category_2)", ascending=False).show()
purchase_train.groupBy("Product_Category_3").agg(f.count("Product_Category_3")).orderBy("count(Product_Category_3)", ascending=False).show()

purchase_test.groupBy("Product_Category_2").agg(f.count("Product_Category_2")).orderBy("count(Product_Category_2)", ascending=False).show()
purchase_test.groupBy("Product_Category_3").agg(f.count("Product_Category_3")).orderBy("count(Product_Category_3)", ascending=False).show()



# fill with cat with most count
purchase_train = purchase_train.na.fill({"Product_Category_2": 8, "Product_Category_3": 16})
purchase_test = purchase_test.na.fill({"Product_Category_2": 8, "Product_Category_3": 16})

# checking
purchase_train.select([f.count(f.when(f.isnull(c), c)).alias(c) for c in purchase_train.columns]).show()
purchase_test.select([f.count(f.when(f.isnull(c), c)).alias(c) for c in purchase_test.columns]).show()

"""CHECKING FOR DISTINCT VALUES"""

# distinct values in each column
purchase_train.agg(*(f.countDistinct(f.col(c)).alias(c) for c in purchase_train.columns)).show()

purchase_test.agg(*(f.countDistinct(f.col(c)).alias(c) for c in purchase_test.columns)).show()

"""Count category values within each of the following column:

 ● Gender

 ● Age

 ● City_Category

 ● Stay_In_Current_City_Years

● **Marital_Status**
"""

#Count gender
purchase_train.groupBy("Gender").agg(f.count("Gender")).orderBy("count(Gender)", ascending=False).show()

#Count age
purchase_train.groupBy("Age").agg(f.count("Age")).orderBy("count(Age)", ascending=False).show()

#Count city_category
purchase_train.groupBy("City_Category").agg(f.count("City_Category")).orderBy("count(City_Category)", ascending=False).show()

#Count Stay_In_Current_City_Years
purchase_train.groupBy("Stay_In_Current_City_Years").agg(f.count("Stay_In_Current_City_Years")).orderBy("count(Stay_In_Current_City_Years)", ascending=False).show()

#Count Marital_status
purchase_train.groupBy("Marital_status").agg(f.count("Marital_Status")).orderBy("count(Marital_Status)", ascending=False).show()

"""Calculate average Purchase for each of the following columns:

 ● Gender

 ● Age

 ● City_Category

 ● Stay_In_Current_City_Years

 ● Marital_Status
"""

# avrage purchase for gender
purchase_train.groupBy("Gender").agg(f.round(f.avg("Purchase"),2).alias("Average_purchase")).orderBy("Average_Purchase", ascending=False).show()

# avrage purchase forage
purchase_train.groupBy("Age").agg(f.round(f.avg("Purchase"),2).alias("Average_purchase")).orderBy("Average_Purchase", ascending=False).show()

# avrage purchase for city category
purchase_train.groupBy("City_Category").agg(f.round(f.avg("Purchase"),2).alias("Average_purchase")).orderBy("Average_Purchase", ascending=False).show()

# avrage purchase for stayer in current city years
purchase_train.groupBy("Stay_In_Current_City_Years").agg(f.round(f.avg("Purchase"),2).alias("Average_purchase")).orderBy("Average_Purchase", ascending=False).show()

# avrage purchase for marital status
purchase_train.groupBy("Marital_Status").agg(f.round(f.avg("Purchase"),2).alias("Average_purchase")).orderBy("Average_Purchase", ascending=False).show()

"""Label encode the following columns:

 ● Age

 ● Gender

 ● Stay_In_Current_City_Years

 ● City_Category
"""

#import libraries for label encoding
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml import pipeline

# label coding
SI_Age = StringIndexer(inputCol="Age", outputCol="Age_indexed", handleInvalid="skip")
SI_Gender = StringIndexer(inputCol="Gender", outputCol="Gender_indexed", handleInvalid="skip")
SI_Stay_In_Current_city_Years = StringIndexer(inputCol="Stay_In_Current_City_Years", outputCol="Stay_indexed", handleInvalid="skip")
SI_city_Category = StringIndexer(inputCol="City_Category", outputCol="City_indexed", handleInvalid="skip")

SI_Age_Obj = SI_Age.fit(purchase_train)
SI_Gender_Obj = SI_Gender.fit(purchase_train)
SI_Stay_In_Current_city_Years_Obj = SI_Stay_In_Current_city_Years.fit(purchase_train)
SI_city_Obj = SI_city_Category.fit(purchase_train)

purchase_train_encoded = SI_Age_Obj.transform(purchase_train)
purchase_train_encoded = SI_Gender_Obj.transform(purchase_train_encoded)
purchase_train_encoded = SI_Stay_In_Current_city_Years_Obj.transform(purchase_train_encoded)
purchase_train_encoded = SI_city_Obj.transform(purchase_train_encoded)

purchase_test_encoded = SI_Age_Obj.transform(purchase_test)
purchase_test_encoded = SI_Gender_Obj.transform(purchase_test_encoded)
purchase_test_encoded = SI_Stay_In_Current_city_Years_Obj.transform(purchase_test_encoded)
purchase_test_encoded = SI_city_Obj.transform(purchase_test_encoded)

purchase_train_encoded.show()

"""One-Hot encode following columns:

 ● Gender

 ● City_Category

 ● Occupation
"""

purchase_train_encoded.columns

OHE_train = OneHotEncoder(inputCols = ["Gender_indexed",
                                       "City_indexed",
                                       "Occupation"],
                          outputCols = ["Gender_ohe",
                                        "City_Category_ohe",
                                        "Occupation_ohe"])

OHE_Obj = OHE_train.fit(purchase_train_encoded)

purchase_train_encoded = OHE_Obj.transform(purchase_train_encoded)
purchase_train_encoded.show()

purchase_test_encoded = OHE_Obj.transform(purchase_test_encoded)

purchase_train_encoded.columns

"""Build a baseline model using any of the ML algorithms."""

assembler = VectorAssembler(
    inputCols=[
        'Age_indexed',
        'Stay_indexed',
        'Product_Category_1',
        'Product_Category_2',
        'Product_Category_3',
        'Marital_Status',
        'Gender_ohe',
        'City_Category_ohe',
        'Occupation_ohe'
    ],
    outputCol='features'
)

purchase_train_encoded = assembler.transform(purchase_train_encoded)
purchase_test_encoded = assembler.transform(purchase_test_encoded)

purchase_train_encoded.select("features").show()

purchase_valid_encoded = purchase_test_encoded

purchase_train, purchase_test = purchase_train_encoded.randomSplit([0.8,0.2], seed=42)

from pyspark.ml.regression import GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator

model_gbt = GBTRegressor(featuresCol="features", labelCol="Purchase")

model_gbt = model_gbt.fit(purchase_train)

purchase_test.selectExpr("min(Purchase)", "max(Purchase)", "avg(Purchase)").show()

predictions_gbt = model_gbt.transform(purchase_train)
evaluator_gbt = RegressionEvaluator(labelCol="Purchase", predictionCol="prediction", metricName="rmse")
print("RMSE:", evaluator_gbt.evaluate(predictions_gbt))

predictions_gbt = model_gbt.transform(purchase_test)
evaluator_gbt = RegressionEvaluator(labelCol="Purchase", predictionCol="prediction", metricName="rmse")
print("RMSE:", evaluator_gbt.evaluate(predictions_gbt))

"""Model improvement with Grid-Search CV"""

from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
paramGrid = ParamGridBuilder() \
    .addGrid(model_gbt.maxDepth, [3, 5, 7]) \
    .addGrid(model_gbt.maxIter, [50, 100]) \
    .addGrid(model_gbt.stepSize, [0.05, 0.1]) \
    .build()

model_gbt = GBTRegressor(featuresCol="features", labelCol="Purchase")

cv = CrossValidator(estimator=model_gbt,
                    estimatorParamMaps=paramGrid,
                    evaluator= evaluator_gbt,
                    numFolds=5,
                    seed=27)

grid_model_gbt = cv.fit(purchase_train)

predictions_gbt = grid_model_gbt.transform(purchase_train)
evaluator_gbt = RegressionEvaluator(labelCol="Purchase", predictionCol="prediction", metricName="rmse")
print("RMSE:", evaluator_gbt.evaluate(predictions_gbt))

predictions_gbt = grid_model_gbt.transform(purchase_test)
evaluator_gbt = RegressionEvaluator(labelCol="Purchase", predictionCol="prediction", metricName="rmse")
print("RMSE:", evaluator_gbt.evaluate(predictions_gbt))

best_model = grid_model_gbt.bestModel
print("Best regParam:", best_model)

print("Best maxIter:", best_model.getMaxIter())
print("Best maxDepth:", best_model.getMaxDepth())
print("Best stepSize:", best_model.getStepSize())
print("Num Trees:", best_model.getNumTrees)
print("Num Features:", best_model.numFeatures)

final_gbt = GBTRegressor(
    featuresCol="features",
    labelCol="Purchase",
    maxIter=20,
    maxDepth=5,
    stepSize=0.1
)

final_model = final_gbt.fit(purchase_train)

predictions = final_model.transform(purchase_test)
evaluator = RegressionEvaluator(labelCol="Purchase", predictionCol="prediction", metricName="rmse")
print("RMSE:", evaluator.evaluate(predictions_gbt))
predictions.select("Purchase", "prediction").show(5)





















"""Create a Spark ML Pipeline for the final model."""

# 1. Index categorical columns
indexers = [
    StringIndexer(inputCol='Gender', outputCol='Gender_indexed', handleInvalid="keep"),
    StringIndexer(inputCol='Age', outputCol='Age_indexed', handleInvalid="keep"),
    StringIndexer(inputCol='Stay_In_Current_City_Years', outputCol='Stay_indexed', handleInvalid="keep"),
    StringIndexer(inputCol='City_Category', outputCol='City_indexed', handleInvalid="keep")
]

# 2. One-Hot Encode selected indexed columns
encoder = OneHotEncoder(
    inputCols=["Gender_indexed", "City_indexed", "Occupation"],
    outputCols=["Gender_ohe", "City_Category_ohe", "Occupation_ohe"],
    handleInvalid="keep"
)

# 3. Feature columns for the model
feature_cols = [
        'Age_indexed',
        'Stay_indexed',
        'Product_Category_1',
        'Product_Category_2',
        'Product_Category_3',
        'Marital_Status',
        'Gender_ohe',
        'City_Category_ohe',
        'Occupation_ohe'
]

assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

# 4. Final GBT Model with tuned params
gbt = GBTRegressor(
    featuresCol="features",
    labelCol="Purchase",
    maxIter=20,
    maxDepth=5,
    stepSize=0.1
)

# 5. Create the pipeline
from pyspark.ml import Pipeline
pipeline = Pipeline(stages=indexers + [encoder, assembler, gbt])

pipeline

#load data again
purchase_train_ag = spark.read.csv("/content/train.csv", header=True, inferSchema=True)
purchase_test_ag = spark.read.csv("/content/test.csv", header=True, inferSchema=True)

purchase_train_ag = purchase_train_ag.na.fill({"Product_Category_2": 8, "Product_Category_3": 16})
purchase_test_ag = purchase_test_ag.na.fill({"Product_Category_2": 8, "Product_Category_3": 16})

purchase_valid_ag = purchase_test_ag

purchase_train_ag, purchase_test_ag = purchase_train_ag.randomSplit([0.8,0.2], seed=42)

# 6. Fit the model on training data
pipeline_model = pipeline.fit(purchase_train_ag)

# 7. Predict on test data
predictions = pipeline_model.transform(purchase_test_ag)
predictions.select("Purchase", "prediction").show(5)

evaluator = RegressionEvaluator(labelCol="Purchase", predictionCol="prediction", metricName="rmse")
print("RMSE:", evaluator.evaluate(predictions))

spark.stop()